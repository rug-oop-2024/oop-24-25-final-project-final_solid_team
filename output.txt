
# ----- requirements.txt -----
streamlit
pydantic
pandas
numpy
scikit-learn

# ----- .gitignore -----
*pth
*vscode
*__pycache__
*__MACOSX
*assets*
env/

.ignore  # for print_project (self made program)

# ----- .pre-commit-config.yaml -----
# .pre-commit-config.yaml
repos:
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.4.0
    hooks:
      - id: trailing-whitespace
      - id: end-of-file-fixer

  - repo: https://github.com/pycqa/flake8
    rev: 7.1.1
    hooks:
      - id: flake8
        additional_dependencies:
          - flake8-docstrings
          - flake8-annotations

  # - repo: https://github.com/pre-commit/mirrors-isort
  #   rev: v5.10.1
  #   hooks:
  #     - id: isort
  #       args: ["--profile", "fl"]

  # - repo: https://github.com/pre-commit/mirrors-mypy
  #   rev: 'v1.11.2'
  #   hooks:
  #     - id: mypy
  #       args:
  #         - --ignore-missing-imports
  #         - --show-error-codes

# ----- test.py -----
#!/bin/env python

import pandas as pd
import numpy as np
from typing import List, Sized, Literal

class Artifact:
    def __init__(self, type, x, y):
        pass

class Dataset(Artifact):

    def __init__(self, *args, **kwargs):
        super().__init__(type="dataset", *args, **kwargs)

if __name__ == "__main__":
    x  = Dataset(x="x", y="y")
# ----- run_tests.sh -----
#!/bin/bash
pre-commit run --files $(find app autoop -type f -name "*.py")

# python3 tests/test_game.py <- Does not work at the moment
python3 -m autoop.tests.main

# ----- autoop/core/database.py -----
import json
from typing import Dict, Tuple, List, Union

from autoop.core.storage import Storage


class Database:

    def __init__(self, storage: Storage):
        self._storage = storage
        self._data = {}
        self._load()

    def set(self, collection: str, id: str, entry: dict) -> dict:
        """Set a key in the database
        Args:
            collection (str): The collection to store the data in
            id (str): The id of the data
            entry (dict): The data to store
        Returns:
            dict: The data that was stored
        """
        assert isinstance(entry, dict), "Data must be a dictionary"
        assert isinstance(collection, str), "Collection must be a string"
        assert isinstance(id, str), "ID must be a string"
        if not self._data.get(collection, None):
            self._data[collection] = {}
        self._data[collection][id] = entry
        self._persist()
        return entry

    def get(self, collection: str, id: str) -> Union[dict, None]:
        """Get a key from the database
        Args:
            collection (str): The collection to get the data from
            id (str): The id of the data
        Returns:
            Union[dict, None]: The data that was stored, or None if it doesn't exist
        """
        if not self._data.get(collection, None):
            return None
        return self._data[collection].get(id, None)

    def delete(self, collection: str, id: str):
        """Delete a key from the database
        Args:
            collection (str): The collection to delete the data from
            id (str): The id of the data
        Returns:
            None
        """
        if not self._data.get(collection, None):
            return
        if self._data[collection].get(id, None):
            del self._data[collection][id]
        self._persist()

    def list(self, collection: str) -> List[Tuple[str, dict]]:
        """Lists all data in a collection
        Args:
            collection (str): The collection to list the data from
        Returns:
            List[Tuple[str, dict]]: A list of tuples containing the id and data for each item in the collection
        """
        if not self._data.get(collection, None):
            return []
        return [(id, data) for id, data in self._data[collection].items()]

    def refresh(self):
        """Refresh the database by loading the data from storage"""
        self._load()

    def _persist(self):
        """Persist the data to storage"""
        for collection, data in self._data.items():
            if not data:
                continue
            for id, item in data.items():
                self._storage.save(
                    json.dumps(item).encode(), f"{collection}/{id}"
                )

        # for things that were deleted, we need to remove them from the storage
        keys = self._storage.list("")
        for key in keys:
            collection, id = key.split("/")[-2:]
            if not self._data.get(collection, id):
                self._storage.delete(f"{collection}/{id}")

    def _load(self):
        """Load the data from storage"""
        self._data = {}
        for key in self._storage.list(""):
            collection, id = key.split("/")[-2:]
            data = self._storage.load(f"{collection}/{id}")
            # Ensure the collection exists in the dictionary
            if collection not in self._data:
                self._data[collection] = {}
            self._data[collection][id] = json.loads(data.decode())

# ----- autoop/core/storage.py -----
from abc import ABC, abstractmethod
import os
from typing import List, Union
from glob import glob


class NotFoundError(Exception):
    def __init__(self, path):
        super().__init__(f"Path not found: {path}")


class Storage(ABC):

    @abstractmethod
    def save(self, data: bytes, path: str):
        """
        Save data to a given path
        Args:
            data (bytes): Data to save
            path (str): Path to save data
        """
        pass

    @abstractmethod
    def load(self, path: str) -> bytes:
        """
        Load data from a given path
        Args:
            path (str): Path to load data
        Returns:
            bytes: Loaded data
        """
        pass

    @abstractmethod
    def delete(self, path: str):
        """
        Delete data at a given path
        Args:
            path (str): Path to delete data
        """
        pass

    @abstractmethod
    def list(self, path: str) -> list:
        """
        List all paths under a given path
        Args:
            path (str): Path to list
        Returns:
            list: List of paths
        """
        pass


class LocalStorage(Storage):

    def __init__(self, base_path: str = "./assets"):
        self._base_path = base_path
        if not os.path.exists(self._base_path):
            os.makedirs(self._base_path)

    def save(self, data: bytes, key: str):
        path = self._join_path(key)
        if not os.path.exists(path):
            os.makedirs(os.path.dirname(path), exist_ok=True)
        with open(path, "wb") as f:
            f.write(data)

    def load(self, key: str) -> bytes:
        path = self._join_path(key)
        self._assert_path_exists(path)
        with open(path, "rb") as f:
            return f.read()

    def delete(self, key: str = "/"):
        self._assert_path_exists(self._join_path(key))
        path = self._join_path(key)
        os.remove(path)

    def list(self, prefix: str) -> List[str]:
        path = self._join_path(prefix)
        self._assert_path_exists(path)
        keys = glob(path + "/**/*", recursive=True)
        return list(filter(os.path.isfile, keys))

    def _assert_path_exists(self, path: str):
        if not os.path.exists(path):
            raise NotFoundError(path)

    def _join_path(self, path: str) -> str:
        return os.path.join(self._base_path, path)

# ----- autoop/core/ml/pipeline.py -----
from typing import List
import pickle

from autoop.core.ml.artifact import Artifact
from autoop.core.ml.dataset import Dataset
from autoop.core.ml.model import Model
from autoop.core.ml.feature import Feature
from autoop.core.ml.metric import Metric
from autoop.functional.preprocessing import preprocess_features
import numpy as np


class Pipeline:

    def __init__(
        self,
        metrics: List[Metric],
        dataset: Dataset,
        model: Model,
        input_features: List[Feature],
        target_feature: Feature,
        split=0.8,
    ):
        self._dataset = dataset
        self._model = model
        self._input_features = input_features
        self._target_feature = target_feature
        self._metrics = metrics
        self._artifacts = {}
        self._split = split
        if (
            target_feature.type == "categorical"
            and model.type != "classification"
        ):
            raise ValueError(
                "Model type must be classification for categorical target feature"
            )
        if target_feature.type == "continuous" and model.type != "regression":
            raise ValueError(
                "Model type must be regression for continuous target feature"
            )

    def __str__(self):
        return f"""
Pipeline(
    model={self._model.type},
    input_features={list(map(str, self._input_features))},
    target_feature={str(self._target_feature)},
    split={self._split},
    metrics={list(map(str, self._metrics))},
)
"""

    @property
    def model(self):
        return self._model  # UNSAFE, user can modify model.

    @property
    def artifacts(self) -> List[Artifact]:
        """Used to get the artifacts generated during the pipeline execution to be saved"""
        artifacts = []
        for name, artifact in self._artifacts.items():
            artifact_type = artifact.get("type")
            if artifact_type in ["OneHotEncoder"]:
                data = artifact["encoder"]
                data = pickle.dumps(data)
                artifacts.append(Artifact(name=name, data=data))
            if artifact_type in ["StandardScaler"]:
                data = artifact["scaler"]
                data = pickle.dumps(data)
                artifacts.append(Artifact(name=name, data=data))
        pipeline_data = {
            "input_features": self._input_features,
            "target_feature": self._target_feature,
            "split": self._split,
        }
        artifacts.append(
            Artifact(name="pipeline_config", data=pickle.dumps(pipeline_data))
        )
        artifacts.append(
            self._model.to_artifact(name=f"pipeline_model_{self._model.type}")
        )
        return artifacts

    def _register_artifact(self, name: str, artifact):
        self._artifacts[name] = artifact

    def _preprocess_features(self):
        (target_feature_name, target_data, artifact) = preprocess_features(
            [self._target_feature], self._dataset
        )[0]
        self._register_artifact(target_feature_name, artifact)
        input_results = preprocess_features(
            self._input_features, self._dataset
        )
        for feature_name, data, artifact in input_results:
            self._register_artifact(feature_name, artifact)
        # Get the input vectors and output vector, sort by feature name for consistency
        self._output_vector = target_data
        self._input_vectors = [
            data for (feature_name, data, artifact) in input_results
        ]

    def _split_data(self):
        # Split the data into training and testing sets
        split = self._split
        self._train_X = [
            vector[: int(split * len(vector))]
            for vector in self._input_vectors
        ]
        self._test_X = [
            vector[int(split * len(vector)) :]
            for vector in self._input_vectors
        ]
        self._train_y = self._output_vector[
            : int(split * len(self._output_vector))
        ]
        self._test_y = self._output_vector[
            int(split * len(self._output_vector)) :
        ]

    def _compact_vectors(self, vectors: List[np.array]) -> np.array:
        return np.concatenate(vectors, axis=1)

    def _train(self):
        X = self._compact_vectors(self._train_X)
        Y = self._train_y
        self._model.fit(X, Y)

    def _evaluate(self):
        X = self._compact_vectors(self._test_X)
        Y = self._test_y
        self._metrics_results = []
        predictions = self._model.predict(X)
        for metric in self._metrics:
            result = metric.evaluate(predictions, Y)
            self._metrics_results.append((metric, result))
        self._predictions = predictions

    def execute(self):
        self._preprocess_features()
        self._split_data()
        self._train()
        self._evaluate()
        return {
            "metrics": self._metrics_results,
            "predictions": self._predictions,
        }

# ----- autoop/core/ml/dataset.py -----
from autoop.core.ml.artifact import Artifact
from abc import ABC, abstractmethod
import pandas as pd
import io


class Dataset(Artifact):

    def __init__(self, *args, **kwargs):
        super().__init__(type="dataset", *args, **kwargs)

    @staticmethod
    def from_dataframe(
        data: pd.DataFrame, name: str, asset_path: str, version: str = "1.0.0"
    ):
        return Dataset(
            name=name,
            asset_path=asset_path,
            data=data.to_csv(index=False).encode(),
            version=version,
        )

    def read(self) -> pd.DataFrame:
        bytes = super().read()
        csv = bytes.decode()
        return pd.read_csv(io.StringIO(csv))

    def save(self, data: pd.DataFrame) -> bytes:
        bytes = data.to_csv(index=False).encode()
        return super().save(bytes)

# ----- autoop/core/ml/metric.py -----
from abc import ABC, abstractmethod
from typing import Any
import numpy as np

METRICS = [
    "mean_squared_error",
    "accuracy",
]  # add the names (in strings) of the metrics you implement


def get_metric(name: str):
    # Factory function to get a metric by name.
    # Return a metric instance given its str name.
    raise NotImplementedError("To be implemented.")


class Metric(...):
    """Base class for all metrics."""

    # your code here
    # remember: metrics take ground truth and prediction as input and return a real number

    def __call__(self):
        raise NotImplementedError("To be implemented.")


# add here concrete implementations of the Metric class

# ----- autoop/core/ml/artifact.py -----
from pydantic import BaseModel, Field
import base64


class Artifact(BaseModel):
    _type: str
    _dictionary: dict

    def __init__(
        self,
        type: str,
        name: str,
        asset_path: str,
        data: str,         # csv file in the form of a string
        version: str
    ):
        """_summary_

        Args:
            type (str): _description_
            name (str): _description_
            asset_path (str): _description_
            data (str): _description_
        """



# Has methods:
# .__init__(type="dataset", *args, **kwargs)
# super().read() --> some kind bytes type
# super().save(bytes)

# ----- autoop/core/ml/feature.py -----
from typing import Literal, Sized
import numpy as np

from autoop.core.ml.dataset import Dataset

OneZero = Literal[0, 1]
Categorical = tuple[OneZero]


class Feature:
    # attributes here
    _type: str

    def __init__(self, data: Sized, type: str):
        self._type = type
        self._data = data

    @property
    def type(self):
        return self._type

    def __str__(self):
        raise NotImplementedError("To be implemented.")

# ----- autoop/core/ml/model/__init__.py -----
from autoop.core.ml.model.model import Model
from autoop.core.ml.model.regression import MultipleLinearRegression

REGRESSION_MODELS = []  # add your models as str here

CLASSIFICATION_MODELS = []  # add your models as str here


def get_model(model_name: str) -> Model:
    """Factory function to get a model by name."""
    raise NotImplementedError("To be implemented.")

# ----- autoop/core/ml/model/model.py -----
from abc import abstractmethod
from autoop.core.ml.artifact import Artifact
import numpy as np
from copy import deepcopy
from typing import Literal


class Model:
    pass  # your code (attribute and methods) here

# ----- autoop/core/ml/model/classification/__init__.py -----

# ----- autoop/core/ml/model/regression/__init__.py -----
from autoop.core.ml.model.regression.multiple_linear_regression import (
    MultipleLinearRegression,
)

# ----- autoop/functional/preprocessing.py -----
from typing import List, Tuple
from autoop.core.ml.feature import Feature
from autoop.core.ml.dataset import Dataset
import pandas as pd
import numpy as np
from sklearn.preprocessing import OneHotEncoder, StandardScaler


def preprocess_features(
    features: List[Feature], dataset: Dataset
) -> List[Tuple[str, np.ndarray, dict]]:
    """Preprocess features.
    Args:
        features (List[Feature]): List of features.
        dataset (Dataset): Dataset object.
    Returns:
        List[str, Tuple[np.ndarray, dict]]: List of preprocessed features. Each ndarray of shape (N, ...)
    """
    results = []
    raw = dataset.read()
    for feature in features:
        if feature.type == "categorical":
            encoder = OneHotEncoder()
            data = encoder.fit_transform(
                raw[feature.name].values.reshape(-1, 1)
            ).toarray()
            aritfact = {
                "type": "OneHotEncoder",
                "encoder": encoder.get_params(),
            }
            results.append((feature.name, data, aritfact))
        if feature.type == "numerical":
            scaler = StandardScaler()
            data = scaler.fit_transform(
                raw[feature.name].values.reshape(-1, 1)
            )
            artifact = {
                "type": "StandardScaler",
                "scaler": scaler.get_params(),
            }
            results.append((feature.name, data, artifact))
    # Sort for consistency
    results = list(sorted(results, key=lambda x: x[0]))
    return results

# ----- autoop/functional/feature.py -----
from typing import List, Any

import pandas as pd
from autoop.core.ml.dataset import Dataset
from autoop.core.ml.feature import Feature


def _is_categorical(candidate: Any):
    if isinstance(candidate, str):
        return True


def detect_feature_types(dataset: Dataset) -> List[Feature]:
    """Assumption: only categorical and numerical features and no NaN values.
    Args:
        dataset: Dataset
    Returns:
        List[Feature]: List of features with their types.
    """
    df = dataset.read()

    features = []
    for column_name in df:
        if all(
            [isinstance(element, (float, int)) for element in df[column_name]]
        ):
            feature = Feature(df[column_name], "numerical")
            features.append(feature)
        if all([_is_categorical(element) for element in df[column_name]]):
            feature = Feature(df[column_name], "categorical")
            features.append(feature)

        # No need to check for NaN types because only floats, ints and strings are added

    return features

# ----- autoop/tests/test_storage.py -----
import unittest

from autoop.core.storage import LocalStorage, NotFoundError
import random
import tempfile


class TestStorage(unittest.TestCase):

    def setUp(self):
        temp_dir = tempfile.mkdtemp()
        self.storage = LocalStorage(temp_dir)

    def test_init(self):
        self.assertIsInstance(self.storage, LocalStorage)

    def test_store(self):
        key = str(random.randint(0, 100))
        test_bytes = bytes([random.randint(0, 255) for _ in range(100)])
        key = "test/path"
        self.storage.save(test_bytes, key)
        self.assertEqual(self.storage.load(key), test_bytes)
        otherkey = "test/otherpath"
        # should not be the same
        try:
            self.storage.load(otherkey)
        except Exception as e:
            self.assertIsInstance(e, NotFoundError)

    def test_delete(self):
        key = str(random.randint(0, 100))
        test_bytes = bytes([random.randint(0, 255) for _ in range(100)])
        key = "test/path"
        self.storage.save(test_bytes, key)
        self.storage.delete(key)
        try:
            self.assertIsNone(self.storage.load(key))
        except Exception as e:
            self.assertIsInstance(e, NotFoundError)

    def test_list(self):
        key = str(random.randint(0, 100))
        test_bytes = bytes([random.randint(0, 255) for _ in range(100)])
        random_keys = [f"test/{random.randint(0, 100)}" for _ in range(10)]
        for key in random_keys:
            self.storage.save(test_bytes, key)
        keys = self.storage.list("test")
        keys = ["/".join(key.split("/")[-2:]) for key in keys]
        self.assertEqual(set(keys), set(random_keys))

# ----- autoop/tests/test_pipeline.py -----
from sklearn.datasets import fetch_openml
import unittest
import pandas as pd

from autoop.core.ml.pipeline import Pipeline
from autoop.core.ml.dataset import Dataset
from autoop.core.ml.feature import Feature
from autoop.functional.feature import detect_feature_types
from autoop.core.ml.model.regression import MultipleLinearRegression
from autoop.core.ml.metric import MeanSquaredError


class TestPipeline(unittest.TestCase):

    def setUp(self) -> None:
        data = fetch_openml(name="adult", version=1, parser="auto")
        df = pd.DataFrame(
            data.data,
            columns=data.feature_names,
        )
        self.dataset = Dataset.from_dataframe(
            name="adult",
            asset_path="adult.csv",
            data=df,
        )
        self.features = detect_feature_types(self.dataset)
        self.pipeline = Pipeline(
            dataset=self.dataset,
            model=MultipleLinearRegression(),
            input_features=list(
                filter(lambda x: x.name != "age", self.features)
            ),
            target_feature=Feature(name="age", type="numerical"),
            metrics=[MeanSquaredError()],
            split=0.8,
        )
        self.ds_size = data.data.shape[0]

    def test_init(self):
        self.assertIsInstance(self.pipeline, Pipeline)

    def test_preprocess_features(self):
        self.pipeline._preprocess_features()
        self.assertEqual(len(self.pipeline._artifacts), len(self.features))

    def test_split_data(self):
        self.pipeline._preprocess_features()
        self.pipeline._split_data()
        self.assertEqual(
            self.pipeline._train_X[0].shape[0], int(0.8 * self.ds_size)
        )
        self.assertEqual(
            self.pipeline._test_X[0].shape[0],
            self.ds_size - int(0.8 * self.ds_size),
        )

    def test_train(self):
        self.pipeline._preprocess_features()
        self.pipeline._split_data()
        self.pipeline._train()
        self.assertIsNotNone(self.pipeline._model.parameters)

    def test_evaluate(self):
        self.pipeline._preprocess_features()
        self.pipeline._split_data()
        self.pipeline._train()
        self.pipeline._evaluate()
        self.assertIsNotNone(self.pipeline._predictions)
        self.assertIsNotNone(self.pipeline._metrics_results)
        self.assertEqual(len(self.pipeline._metrics_results), 1)

# ----- autoop/tests/test_database.py -----
import unittest

from autoop.core.database import Database
from autoop.core.storage import LocalStorage
import random
import tempfile


class TestDatabase(unittest.TestCase):

    def setUp(self):
        self.storage = LocalStorage(tempfile.mkdtemp())
        self.db = Database(self.storage)

    def test_init(self):
        self.assertIsInstance(self.db, Database)

    def test_set(self):
        id = str(random.randint(0, 100))
        entry = {"key": random.randint(0, 100)}
        self.db.set("collection", id, entry)
        self.assertEqual(self.db.get("collection", id)["key"], entry["key"])

    def test_delete(self):
        id = str(random.randint(0, 100))
        value = {"key": random.randint(0, 100)}
        self.db.set("collection", id, value)
        self.db.delete("collection", id)
        self.assertIsNone(self.db.get("collection", id))
        self.db.refresh()
        self.assertIsNone(self.db.get("collection", id))

    def test_persistance(self):
        id = str(random.randint(0, 100))
        value = {"key": random.randint(0, 100)}
        self.db.set("collection", id, value)
        other_db = Database(self.storage)
        self.assertEqual(other_db.get("collection", id)["key"], value["key"])

    def test_refresh(self):
        key = str(random.randint(0, 100))
        value = {"key": random.randint(0, 100)}
        other_db = Database(self.storage)
        self.db.set("collection", key, value)
        other_db.refresh()
        self.assertEqual(other_db.get("collection", key)["key"], value["key"])

    def test_list(self):
        key = str(random.randint(0, 100))
        value = {"key": random.randint(0, 100)}
        self.db.set("collection", key, value)
        # collection should now contain the key
        self.assertIn((key, value), self.db.list("collection"))

# ----- autoop/tests/test_features.py -----
import unittest
from unittest.mock import patch
from sklearn.datasets import load_iris, fetch_openml
import pandas as pd

from autoop.core.ml.dataset import Dataset
from autoop.core.ml.feature import Feature
from autoop.functional.feature import detect_feature_types


class TestFeatures(unittest.TestCase):

    def setUp(self) -> None:
        pass

    def test_unit_detect_features(self):
        data_set = Dataset()
        ints = pd.DataFrame(
            columns=["ints", "categories", "floats"],
            data=[
                [1, "a", 0.1],
                [2, "b", 0.2],
            ],
        )
        with patch("autoop.core.ml.dataset.Dataset.read") as read_mock:
            with patch("builtins.print") as print_mock:
                read_mock.return_value = ints
                features = detect_feature_types(data_set)
        self.assertEqual(features[0].type, "numerical")
        self.assertEqual(features[1].type, "categorical")
        self.assertEqual(features[2].type, "numerical")

    # def test_detect_features_continuous(self):
    #     iris = load_iris()
    #     df = pd.DataFrame(
    #         iris.data,
    #         columns=iris.feature_names,
    #     )
    #     dataset = Dataset.from_dataframe(
    #         name="iris",
    #         asset_path="iris.csv",
    #         data=df,
    #     )
    #     self.X = iris.data
    #     self.y = iris.target
    #     features = detect_feature_types(dataset)
    #     self.assertIsInstance(features, list)
    #     self.assertEqual(len(features), 4)
    #     for feature in features:
    #         self.assertIsInstance(feature, Feature)
    #         self.assertEqual(feature.name in iris.feature_names, True)
    #         self.assertEqual(feature.type, "numerical")

    # def test_detect_features_with_categories(self):
    #     data = fetch_openml(name="adult", version=1, parser="auto")
    #     df = pd.DataFrame(
    #         data.data,
    #         columns=data.feature_names,
    #     )
    #     dataset = Dataset.from_dataframe(
    #         name="adult",
    #         asset_path="adult.csv",
    #         data=df,
    #     )
    #     features = detect_feature_types(dataset)
    #     self.assertIsInstance(features, list)
    #     self.assertEqual(len(features), 14)
    #     numerical_columns = [
    #         "age",
    #         "education-num",
    #         "capital-gain",
    #         "capital-loss",
    #         "hours-per-week",
    #     ]
    #     categorical_columns = [
    #         "workclass",
    #         "education",
    #         "marital-status",
    #         "occupation",
    #         "relationship",
    #         "race",
    #         "sex",
    #         "native-country",
    #     ]
    #     for feature in features:
    #         self.assertIsInstance(feature, Feature)
    #         self.assertEqual(feature.name in data.feature_names, True)
    #     for detected_feature in filter(lambda x: x.name in numerical_columns, features):
    #         self.assertEqual(detected_feature.type, "numerical")
    #     for detected_feature in filter(lambda x: x.name in categorical_columns, features):
    #         self.assertEqual(detected_feature.type, "categorical")

# ----- autoop/tests/main.py -----
import unittest

# from autoop.tests.test_database import TestDatabase
# from autoop.tests.test_storage import TestStorage
from autoop.tests.test_features import TestFeatures

# from autoop.tests.test_pipeline import TestPipeline

if __name__ == "__main__":
    unittest.main()

# ----- docs/questions.txt -----
GW: Will a dataset contain just the data or also already denote the training/validation split?
GW: What is the difference between a database and a dataset?
# ----- docs/decisions.txt -----
GW: Decided to first implement detect_feature_types despite the fact that it's parameter type is not defined yet.

# ----- docs/assumptions.txt -----
GW: One Feature object is a list of [Number | Categorical] plus maybe some extra functionality - True

GW: The data key in the dict of an artifact actually refers to the data, not just the type of the data.

# ----- app/Welcome.py -----
from autoop.core.ml.artifact import Artifact
import streamlit as st

st.set_page_config(
    page_title="Hello",
    page_icon="👋",
)
st.sidebar.success("Select a page above.")
st.markdown(open("README.md").read())

# ----- app/core/system.py -----
from autoop.core.storage import LocalStorage
from autoop.core.database import Database
from autoop.core.ml.dataset import Dataset
from autoop.core.ml.artifact import Artifact
from autoop.core.storage import Storage
from typing import List


class ArtifactRegistry:
    def __init__(self, database: Database, storage: Storage):
        self._database = database
        self._storage = storage

    def register(self, artifact: Artifact):
        # save the artifact in the storage
        self._storage.save(artifact.data, artifact.asset_path)
        # save the metadata in the database
        entry = {
            "name": artifact.name,
            "version": artifact.version,
            "asset_path": artifact.asset_path,
            "tags": artifact.tags,
            "metadata": artifact.metadata,
            "type": artifact.type,
        }
        self._database.set(f"artifacts", artifact.id, entry)

    def list(self, type: str = None) -> List[Artifact]:
        entries = self._database.list("artifacts")
        artifacts = []
        for id, data in entries:
            if type is not None and data["type"] != type:
                continue
            artifact = Artifact(
                name=data["name"],
                version=data["version"],
                asset_path=data["asset_path"],
                tags=data["tags"],
                metadata=data["metadata"],
                data=self._storage.load(data["asset_path"]),
                type=data["type"],
            )
            artifacts.append(artifact)
        return artifacts

    def get(self, artifact_id: str) -> Artifact:
        data = self._database.get("artifacts", artifact_id)
        return Artifact(
            name=data["name"],
            version=data["version"],
            asset_path=data["asset_path"],
            tags=data["tags"],
            metadata=data["metadata"],
            data=self._storage.load(data["asset_path"]),
            type=data["type"],
        )

    def delete(self, artifact_id: str):
        data = self._database.get("artifacts", artifact_id)
        self._storage.delete(data["asset_path"])
        self._database.delete("artifacts", artifact_id)


class AutoMLSystem:
    _instance = None

    def __init__(self, storage: LocalStorage, database: Database):
        self._storage = storage
        self._database = database
        self._registry = ArtifactRegistry(database, storage)

    @staticmethod
    def get_instance():
        if AutoMLSystem._instance is None:
            AutoMLSystem._instance = AutoMLSystem(
                LocalStorage("./assets/objects"),
                Database(LocalStorage("./assets/dbo")),
            )
        AutoMLSystem._instance._database.refresh()
        return AutoMLSystem._instance

    @property
    def registry(self):
        return self._registry

# ----- app/pages/1_📊_Datasets.py -----
import streamlit as st
import pandas as pd

from app.core.system import AutoMLSystem
from autoop.core.ml.dataset import Dataset

automl = AutoMLSystem.get_instance()

datasets = automl.registry.list(type="dataset")

# your code here

# ----- app/pages/3_Readme.py -----
# from autoop.core.ml.artifact import Artifact
import streamlit as st

st.set_page_config(
    page_title="Readme",
    page_icon="👋",
)

st.markdown(open("README.md").read())

# ----- app/pages/0_✅_Instructions.py -----
# from autoop.core.ml.artifact import Artifact
import streamlit as st

st.set_page_config(
    page_title="Instructions",
    page_icon="👋",
)

st.markdown(open("INSTRUCTIONS.md").read())

# ----- app/pages/2_⚙_Modelling.py -----
import streamlit as st
import pandas as pd

from app.core.system import AutoMLSystem
from autoop.core.ml.dataset import Dataset


st.set_page_config(page_title="Modelling", page_icon="📈")


def write_helper_text(text: str):
    st.write(f'<p style="color: #888;">{text}</p>', unsafe_allow_html=True)


st.write("# ⚙ Modelling")
write_helper_text(
    "In this section, you can design a machine learning pipeline to train a model on a dataset."
)

automl = AutoMLSystem.get_instance()

datasets = automl.registry.list(type="dataset")

# your code here
